{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO54v1brTHebJ6O2mt5TLVh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thanuja-007/webmining-/blob/main/ROBOTICS_PATHCOVRAGE%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_X86RqCe_XQf",
        "outputId": "eb8885ef-9727-42a5-d3b6-52e4fc9a526b"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 1, Total Reward: 82, Epsilon: 1.00\n",
            "Episode: 2, Total Reward: -143, Epsilon: 0.56\n",
            "Episode: 3, Total Reward: -121, Epsilon: 0.23\n",
            "Episode: 4, Total Reward: -13, Epsilon: 0.20\n",
            "Episode: 5, Total Reward: -189, Epsilon: 0.15\n",
            "Episode: 6, Total Reward: 30, Epsilon: 0.11\n",
            "Episode: 7, Total Reward: -376, Epsilon: 0.08\n",
            "Episode: 8, Total Reward: 23, Epsilon: 0.05\n",
            "Episode: 9, Total Reward: 90, Epsilon: 0.05\n",
            "Episode: 10, Total Reward: 91, Epsilon: 0.05\n",
            "Episode: 11, Total Reward: 93, Epsilon: 0.05\n",
            "Episode: 12, Total Reward: 93, Epsilon: 0.04\n",
            "Episode: 13, Total Reward: 91, Epsilon: 0.04\n",
            "Episode: 14, Total Reward: 93, Epsilon: 0.04\n",
            "Episode: 15, Total Reward: 91, Epsilon: 0.04\n",
            "Episode: 16, Total Reward: 91, Epsilon: 0.04\n",
            "Episode: 17, Total Reward: 86, Epsilon: 0.03\n",
            "Episode: 18, Total Reward: -233, Epsilon: 0.01\n",
            "Episode: 19, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 20, Total Reward: -549, Epsilon: 0.01\n",
            "Episode: 21, Total Reward: 40, Epsilon: 0.01\n",
            "Episode: 22, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 23, Total Reward: 67, Epsilon: 0.01\n",
            "Episode: 24, Total Reward: 39, Epsilon: 0.01\n",
            "Episode: 25, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 26, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 27, Total Reward: 89, Epsilon: 0.01\n",
            "Episode: 28, Total Reward: 91, Epsilon: 0.01\n",
            "Episode: 29, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 30, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 31, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 32, Total Reward: 73, Epsilon: 0.01\n",
            "Episode: 33, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 34, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 35, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 36, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 37, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 38, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 39, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 40, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 41, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 42, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 43, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 44, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 45, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 46, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 47, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 48, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 49, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 50, Total Reward: 91, Epsilon: 0.01\n",
            "Episode: 51, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 52, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 53, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 54, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 55, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 56, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 57, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 58, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 59, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 60, Total Reward: 92, Epsilon: 0.01\n",
            "Episode: 61, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 62, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 63, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 64, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 65, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 66, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 67, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 68, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 69, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 70, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 71, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 72, Total Reward: 92, Epsilon: 0.01\n",
            "Episode: 73, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 74, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 75, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 76, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 77, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 78, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 79, Total Reward: 91, Epsilon: 0.01\n",
            "Episode: 80, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 81, Total Reward: 91, Epsilon: 0.01\n",
            "Episode: 82, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 83, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 84, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 85, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 86, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 87, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 88, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 89, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 90, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 91, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 92, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 93, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 94, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 95, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 96, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 97, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 98, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 99, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 100, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 101, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 102, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 103, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 104, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 105, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 106, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 107, Total Reward: 92, Epsilon: 0.01\n",
            "Episode: 108, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 109, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 110, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 111, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 112, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 113, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 114, Total Reward: 92, Epsilon: 0.01\n",
            "Episode: 115, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 116, Total Reward: 56, Epsilon: 0.01\n",
            "Episode: 117, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 118, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 119, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 120, Total Reward: 92, Epsilon: 0.01\n",
            "Episode: 121, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 122, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 123, Total Reward: 92, Epsilon: 0.01\n",
            "Episode: 124, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 125, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 126, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 127, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 128, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 129, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 130, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 131, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 132, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 133, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 134, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 135, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 136, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 137, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 138, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 139, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 140, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 141, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 142, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 143, Total Reward: 92, Epsilon: 0.01\n",
            "Episode: 144, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 145, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 146, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 147, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 148, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 149, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 150, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 151, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 152, Total Reward: 92, Epsilon: 0.01\n",
            "Episode: 153, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 154, Total Reward: 92, Epsilon: 0.01\n",
            "Episode: 155, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 156, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 157, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 158, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 159, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 160, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 161, Total Reward: 92, Epsilon: 0.01\n",
            "Episode: 162, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 163, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 164, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 165, Total Reward: 92, Epsilon: 0.01\n",
            "Episode: 166, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 167, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 168, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 169, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 170, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 171, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 172, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 173, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 174, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 175, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 176, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 177, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 178, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 179, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 180, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 181, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 182, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 183, Total Reward: 92, Epsilon: 0.01\n",
            "Episode: 184, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 185, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 186, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 187, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 188, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 189, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 190, Total Reward: 91, Epsilon: 0.01\n",
            "Episode: 191, Total Reward: 91, Epsilon: 0.01\n",
            "Episode: 192, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 193, Total Reward: 91, Epsilon: 0.01\n",
            "Episode: 194, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 195, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 196, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 197, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 198, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 199, Total Reward: 93, Epsilon: 0.01\n",
            "Episode: 200, Total Reward: 93, Epsilon: 0.01\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Simplified Grid World\n",
        "class GridWorld:\n",
        "    def __init__(self, size=5, num_agents=1, num_obstacles=3):\n",
        "        self.size = size\n",
        "        self.num_agents = num_agents\n",
        "        self.num_obstacles = num_obstacles\n",
        "        self.agent_positions = [(0, 0)]  # Fixed start position\n",
        "        self.obstacle_positions = [(1, 1), (2, 3), (3, 2)]  # Fixed obstacles\n",
        "        self.target = (size-1, size-1)\n",
        "        self.state_shape = (size, size, 1)\n",
        "\n",
        "    def reset(self):\n",
        "        self.agent_positions = [(0, 0)]\n",
        "        return self.get_state()\n",
        "\n",
        "    def get_state(self):\n",
        "        state = np.zeros((self.size, self.size))\n",
        "        for pos in self.obstacle_positions:\n",
        "            state[pos] = -1  # Obstacle\n",
        "        for agent in self.agent_positions:\n",
        "            state[agent] = 1  # Agents\n",
        "        state[self.target] = 2  # Target\n",
        "        return np.expand_dims(state, axis=-1)\n",
        "\n",
        "    def step(self, action):\n",
        "        x, y = self.agent_positions[0]\n",
        "        # Action mapping: 0=Up, 1=Down, 2=Left, 3=Right\n",
        "        if action == 0: x = max(0, x-1)\n",
        "        elif action == 1: x = min(self.size-1, x+1)\n",
        "        elif action == 2: y = max(0, y-1)\n",
        "        elif action == 3: y = min(self.size-1, y+1)\n",
        "\n",
        "        new_pos = (x, y)\n",
        "        done = False  # Initialize done here\n",
        "\n",
        "        if new_pos in self.obstacle_positions:\n",
        "            reward = -10  # Penalty for hitting obstacle\n",
        "        else:\n",
        "            self.agent_positions[0] = new_pos\n",
        "            if new_pos == self.target:\n",
        "                reward = 100  # Large reward for reaching target\n",
        "                done = True\n",
        "            else:\n",
        "                reward = -1  # Small penalty for each step\n",
        "\n",
        "        return self.get_state(), reward, done\n",
        "\n",
        "# Simplified DQN Agent\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_shape, action_size):\n",
        "        self.state_shape = state_shape\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=1000)  # Smaller memory\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = Sequential([\n",
        "            Flatten(input_shape=self.state_shape),\n",
        "            Dense(32, activation='relu'),  # Smaller network\n",
        "            Dense(self.action_size, activation='linear')\n",
        "        ])\n",
        "        model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
        "        return model\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        q_values = self.model.predict(state, verbose=0)\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def train(self, batch_size=32):\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        states = np.array([t[0][0] for t in minibatch])\n",
        "        next_states = np.array([t[3][0] for t in minibatch])\n",
        "\n",
        "        # Batch prediction for efficiency\n",
        "        targets = self.model.predict(states, verbose=0)\n",
        "        next_q_values = self.model.predict(next_states, verbose=0)\n",
        "\n",
        "        for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n",
        "            if done:\n",
        "                targets[i][action] = reward\n",
        "            else:\n",
        "                targets[i][action] = reward + self.gamma * np.max(next_q_values[i])\n",
        "\n",
        "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "# Training\n",
        "env = GridWorld(size=5, num_agents=1, num_obstacles=3)\n",
        "agent = DQNAgent(env.state_shape, 4)\n",
        "episodes = 200  # Fewer episodes\n",
        "\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, (1, *env.state_shape))\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done = env.step(action)\n",
        "        next_state = np.reshape(next_state, (1, *env.state_shape))\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        agent.train()\n",
        "\n",
        "    print(f\"Episode: {episode+1}, Total Reward: {total_reward}, Epsilon: {agent.epsilon:.2f}\")\n",
        "\n",
        "print(\"Training complete!\")"
      ]
    }
  ]
}